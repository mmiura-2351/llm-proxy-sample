services:
  # llama.cpp server - Qwen2.5 0.5B チャットモデル (CPU専用)
  llama-cpp:
    image: ghcr.io/abetlen/llama-cpp-python:latest
    container_name: llama-cpp
    ports:
      - "8000:8000"
    volumes:
      - llama-models:/models
    environment:
      - MODEL=/models/qwen2.5-0.5b-instruct-q4_k_m.gguf
      - HOST=0.0.0.0
      - PORT=8000
    command: >
      --model /models/qwen2.5-0.5b-instruct-q4_k_m.gguf
      --host 0.0.0.0
      --port 8000
      --n_ctx 2048
      --n_threads 4
    restart: unless-stopped

  # TEI service - Multilingual E5 埋め込みモデル (CPU専用)
  embedding:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.2
    container_name: embedding
    ports:
      - "8001:80"
    volumes:
      - embedding-cache:/data
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    command: >
      --model-id intfloat/multilingual-e5-large-instruct
      --port 80
    restart: unless-stopped

  # LiteLLM Proxy - 統合エンドポイント
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm
    ports:
      - "4000:4000"
    volumes:
      - ./litellm-config.yaml:/app/config.yaml
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
    command: --config /app/config.yaml --port 4000
    depends_on:
      - llama-cpp
      - embedding
    restart: unless-stopped

  # クライアントコンテナ
  client:
    build:
      context: ./client
      dockerfile: Dockerfile
    container_name: client
    volumes:
      - ./client:/app
    environment:
      - LITELLM_API_KEY=${LITELLM_MASTER_KEY:-sk-1234}
    depends_on:
      - litellm
    command: tail -f /dev/null
    restart: unless-stopped

volumes:
  llama-models:
  embedding-cache:
